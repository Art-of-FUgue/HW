{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa647ea",
   "metadata": {},
   "source": [
    "For this notebook you will need\n",
    "\n",
    "+ numpy\n",
    "+ matplotlib\n",
    "+ scipy\n",
    "+ scikit-learn\n",
    "\n",
    "$$\n",
    "\\let\\vaccent=\\v % rename builtin command \\v{} to \\vaccent{}\n",
    "\\renewcommand{\\v}[1]{{\\mathbf{#1}}} % for vectors\n",
    "\\newcommand{\\gv}[1]{{\\mbox{\\boldmath$ #1 $}}} \n",
    "\\newcommand{\\uv}[1]{{\\mathbf{\\hat{#1}}}} % for unit vector\n",
    "\\newcommand{\\abs}[1]{\\left| #1 \\right|} % for absolute value\n",
    "\\newcommand{\\avg}[1]{\\left< #1 \\right>} % for average\n",
    "\\let\\underdot=\\d % rename builtin command \\d{} to \\underdot{}\n",
    "\\renewcommand{\\d}[2]{\\frac{d #1}{d #2}} % for derivatives\n",
    "\\newcommand{\\dd}[2]{\\frac{d^2 #1}{d #2^2}} % for double derivatives\n",
    "% \\newcommand{\\pd}[2]{\\frac{\\partial #1}{\\partial #2}} \n",
    "% \\renewcommand\\eqref[1]{Eq.\\;\\ref{#1}} % new version of eqref\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee03d2d-906b-4dee-aee1-b48f2a17a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929aa97a-fa52-4629-b050-ab5479da6701",
   "metadata": {},
   "source": [
    "# Eigensystem calculations, optimization, and linear dynamical systems\n",
    "\n",
    "### Finding eigenspectra is difficult for large matrices\n",
    "\n",
    "Suppose we are given a square matrix $A \\in \\mathbb{R}^{N \\times N}$. How can we find the eigenvalues and eigenvectors of this matrix numerically? The schoolyard method for performing this calculation consists of first solving the characteristic equation for the eigenvalues $\\lambda$ such that\n",
    "$$\n",
    "\\det(A - \\lambda \\mathbb I) = 0.\n",
    "$$\n",
    "In principle, this equation always factors into a polynomial with solutions corresponding to the eigenvalues $(\\lambda - \\lambda_1)(\\lambda - \\lambda_2)...(\\lambda - \\lambda_N) = 0$. However, as $N$ grows larger, it becomes progressively more difficult to factor the equation and find the roots $\\lambda_i$, since the polynomial has order $\\mathcal{O}(\\lambda^N)$. At large $N$, it often becomes impractical to use numerical root-finding to solve the eigenvalue problem using schoolyard methods.\n",
    "\n",
    "### The power method\n",
    "\n",
    "The [power method](https://www.cs.huji.ac.il/w~csip/tirgul2.pdf) is a purely iterative method for finding the leading eigenvector of large matrices. The basic algorithm is as follows:\n",
    "\n",
    "0. We start with a matrix $A \\in \\mathbb{R}^{N \\times N}$.\n",
    "1. Pick a random vector $\\v{v} \\in \\mathbb{R}^{N}$, and convert it to a unit vector by scaling it with its own norm $\\v{v} \\leftarrow \\v{v}/{||\\v{v}||}$.\n",
    "2. Compute the matrix product of our matrix $A$ with the random unit vector, and then update the vector $\\v{v} \\leftarrow A \\v{v}$.\n",
    "3. Re-normalize the resulting vector, producing a new unit vector, $\\v{v} \\leftarrow \\v{v}/{||\\v{v}||}$\n",
    "4. Repeat steps 2 and 3 until the elements of the output unit vector fluctuate less than a pre-specified tolerance\n",
    "5. Multiply the resulting vector by the original matrix $A$. The length of the resulting vector gives the magnitude of the leading eigenvalue\n",
    "\n",
    "### The power method derivation\n",
    "\n",
    "Suppose we seek to find the leading eigenvector of a matrix $A$. If the matrix $A$ is non-singular, then it has a full-rank eigenbasis $V \\in \\mathbb{R}^{n \\times n}$, spanned by the $N$ independent, orthonormal eigenvectors $\\uv{v}_i$ such that $\\uv{v}_i \\cdot \\uv{v}_j = \\delta_{ij}$. We start with a our random vector $\\v{w}$, and write it in terms of the basis $V$,\n",
    "$$\n",
    "\\v{w} = w_1 \\uv{v}_1 + w_2 \\uv{v}_2 + ... + w_N \\uv{v}_N\n",
    "$$\n",
    "$$\n",
    "A \\v{w} = \\lambda_1 w_1 \\uv{v}_i + \\lambda_2 w_2 \\uv{v}_2 + ... + \\lambda_N w_N \\uv{v}_N\n",
    "$$\n",
    "\n",
    "We next compute the norm of the output vector\n",
    "\n",
    "$$\n",
    "A \\v{w} \\cdot  A \\v{w} = \\lambda_1^2 w_1^2 + \\lambda_1^2 w_1^2 + ... + \\lambda_1^N w_1^N\n",
    "$$\n",
    "for simplicity, we define $C \\equiv \\sqrt{\\lambda_1^2 w_1^2 + \\lambda_1^2 w_1^2 + ... + \\lambda_1^N w_1^N}$. Rescaling our transformed vector by the norm, we apply the matrix $A$ again,\n",
    "$$\n",
    "A \\cdot (A \\cdot \\v{w})/C = (1/C) (\\lambda_1^2 w_1 \\uv{v}_i + \\lambda_2^2 w_2 \\uv{v}_2 + ... + \\lambda_N^2 w_N \\uv{v}_N)\n",
    "$$\n",
    "\n",
    "This quantity has the norm,\n",
    "$$\n",
    "||A \\cdot (A \\cdot \\v{w})/C||^2 = (1/C^2) (\\lambda_1^4 w_1^2 \\uv{v}_i + \\lambda_2^4 w_2 \\uv{v}_2 + ... + \\lambda_N^4 w_N \\uv{v}_N)\n",
    "$$\n",
    "\n",
    "Now we consider the limit as as $M \\rightarrow \\infty$. Without loss of generality, we assume that the $N$ eigenvalues of $A$ are ordered by their magnitude, $\\abs{\\lambda_1} > \\abs{\\lambda_2} > ... > \\abs{\\lambda_N}$. The series above diverges geometrically as we iterate repeatedly, such that\n",
    "\n",
    "$$\n",
    "A^M \\v{w} \\approx \\dfrac{\\lambda_1^M w_1 + ...}{\\sqrt{\\lambda_1^{2M} w_1^2 + ...}} \\uv{v}_1 = \\uv{v_1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## To Do\n",
    "\n",
    "1. Implement the power method in Python. I've included my starter code below.\n",
    "2. Sometimes you'll notice that the power method fails to converge to the correct solution. What is special about randomly-sampled matrices where this occurs? How does the direction of the starting vector affect the time it takes to reach a solution? \n",
    "2. Suppose that we interpret a given linear matrix $X$ as describing a discrete-time linear dynamical system, $\\v{y}_{t+1} = X \\v{y}_t$. What kind of dynamics does the power method exhibit? What about the pathological cases you discussed in the previous solution?\n",
    "5. The power method represents a basic optimization problem, where we are searching for a convergent solution. We saw that our method occasionally fails to find the correct solution. One way to improve our optimization would be to add a momentum term of the form $$\\v{y}_t \\leftarrow \\gamma \\v{y}_{t - 1} + (1 - \\gamma) \\dfrac{X \\v{y}_{t - 1}} {\\abs{X \\v{y}_{t - 1}}} $$. Where $\\gamma \\in (0, 1]$. How would you modify your implementation of the power method, in order to allow momentum? What kinds of pathological dynamics would the momentum term help us avoid?\n",
    "6. Similar to the momentum term, there is also a way to add additional damping to the update rule. What kinds of dynamics would that help us avoid?\n",
    "\n",
    "## Follow-up ideas and additional information\n",
    "\n",
    "+ The runtime complexity of root-finding for a polynomial of the order, $N$ is poorly-defined as a function of $N$, because there are no guaranteed methods for high-order polynomials other than iterative root-finding. A [good guess](https://cs.stackexchange.com/questions/46920/bss-model-computational-complexity-of-finding-the-roots-of-a-polyomial) would be runtime $\\mathcal{O}(N)$ with a large prefactor (for each of $N$ roots, perform a line search, discover a solution, and then reduce the polynomial in degree). However, because finding the initial determinant has runtime $\\mathcal{O}(N^3)$, the overall runtime of this method is still unfavorable compared to iterative methods.\n",
    "+ The power method gives us the leading eigenvalue and eigenvector of a matrix. What about the full eigenspectrum of the matrix? A more sophisticated version of the power method is the so-called [QR algorithm](https://en.wikipedia.org/wiki/QR_algorithm). Recall how, at each step, we renormalized our eigenvector estimate. If we instead propagate a bundle of random vectors (in order to estimate all of the different eigenvectors), the equivalent of renormalization would be repeated orthonormalization via a mechanism such as Gram-Schmidt. The QR algorithm basically performs this iterative re-orthonormalization very quickly via the QR factorization of the bundle.\n",
    "+ We just described an iterative algorithm for finding eigenvalues, and we discussed one improvement via the addition of momentum term. It turns out there's lots of other ways to improve our update rule, such as by adjusted the eigenvector estimate at a rate proportional to the previous change (a gradient). Because gradient descent is used to train modern neural networks, there is a lot of research describing the convergence properties of various update rules; [here's a nice list of some common methods](https://ruder.io/optimizing-gradient-descent/index.html). If you want to further improve your power method implementation, try using some of these more modern rules instead.\n",
    "+ We made an analogy between the iterative power method and the action of a linear dynamical system. For nonlinear dynamical systems, [an algorithm very similar to the power method](https://link.springer.com/article/10.1007/BF02128237) is used to calculate the [Lyapunov exponents](http://www.scholarpedia.org/article/Lyapunov_exponent), which quantify the degree of chaos present in the system. On some level, we can think of linear dynamical systems as linear dynamical systems where the elements of the matrix $A$ change with time and position.\n",
    "+ Oftentimes in scientific computing we encounter rectangular matrices, $A \\in \\mathbb{R}^{N \\times M}$, $M \\neq N$. For these systems the eigenvectors are not defined, and the schoolyard polynomial equation for the eigenvalues will be overdetermined ($N > M$) or underdetermined ($N < M$). However, we can instead calculate the eigenspectrum of the square matrices $A^\\dagger A$ or $A A^\\dagger$, where the dagger indicates the conjugate transpose operation. We refer to the eigenvectors as the \"right\" or \"left\" singular vectors, respectively, and their associated eigenvalues are called the singular values. This concept results in the popular singular value decomposition for non-square matrices: $$ A = U \\Sigma V^\\dagger $$ where the columns of $U$ are the right singular vectors, the columns of $V$ are the left singular vectors, and $\\Sigma$ is a square diagonal matrix with the eigenvalues along the diagonal. Since the eigenvalues and eigenvectors can be listed in any order, by convention we normally list the elements in descending order of eigenvalue magnitude.\n",
    "+ Future work, which might be added to this assignment in a future iteration of the course: do a complexity scaling calculation comparing schoolyard and power method, to see which $N$ they crossover at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4801d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.498457835352376\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SpectralDecompositionPowerMethod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# mm += mm.T # force hermitian\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcond(mm\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSpectralDecompositionPowerMethod\u001b[49m(store_intermediate_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(mm);\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPower method solution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39msingular_values_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SpectralDecompositionPowerMethod' is not defined"
     ]
    }
   ],
   "source": [
    "## import William's solutions\n",
    "# from solutions.eigen import SpectralDecompositionPowerMethod\n",
    "\n",
    "## Use the default eigensystem calculator in numpy as a point of comparison\n",
    "def eigmax_numpy(A):\n",
    "    \"\"\"\n",
    "    Compute the maximum eigenvalue and associated eigenvector in a matrix with Numpy.\n",
    "    \"\"\"\n",
    "    eigsys = np.linalg.eig(A)\n",
    "    ind = np.abs(eigsys[0]).argmax()\n",
    "    return np.real(eigsys[0][ind]), np.real(eigsys[1][:, ind])\n",
    "\n",
    "\n",
    "np.random.seed(2) # for reproducibility\n",
    "mm = np.random.random(size=(10, 10)) / 100\n",
    "mm = np.random.normal(size=(10, 10))# / 100 # these matrices fail to converge more often\n",
    "\n",
    "# mm += mm.T # force hermitian\n",
    "\n",
    "print(np.linalg.cond(mm.T))\n",
    "model = SpectralDecompositionPowerMethod(store_intermediate_results=True, gamma=0.0)\n",
    "model.fit(mm);\n",
    "\n",
    "\n",
    "print(f\"Power method solution: {model.singular_values_}\")\n",
    "print(f\"Numpy solution: {eigmax_numpy(mm)[0]}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model.stored_eigenvalues)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Eigenvalue estimate\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eigmax_numpy(mm)[1], model.components_, '.')\n",
    "plt.xlabel(\"Numpy eigenvector\")\n",
    "plt.ylabel(\"Power method eigenvector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555fdbc-413a-45d1-af62-a66499c0a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "class SpectralDecompositionPowerMethod:\n",
    "    \"\"\"\n",
    "    Store the output vector in the object attribute self.components_ and the \n",
    "    associated eigenvalue in the object attribute self.singular_values_ \n",
    "    \n",
    "    Why this code structure and attribute names? We are using the convention used by \n",
    "    the popular scikit-learn machine learning library:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "    Parameters\n",
    "        max_iter (int): maximum number of iterations to for the calculation\n",
    "        tolerance (float): fractional change in solution to stop iteration early\n",
    "        gamma (float): momentum parameter for the power method\n",
    "        random_state (int): random seed for reproducibility\n",
    "        store_intermediate_results (bool): whether to store the intermediate results as\n",
    "            the power method iterates\n",
    "        stored_eigenvalues (list): If store_intermediate_results is active, a list of \n",
    "            eigenvalues at each iteration\n",
    "        stored_eigenvectors (list): If store_intermediate_results is active, a list of\n",
    "            eigenvectors at each iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        max_iter=1000, \n",
    "        tolerance=1e-5, \n",
    "        gamma=0.0,\n",
    "        random_state=None, \n",
    "        store_intermediate_results=False\n",
    "    ):\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "        self.store_intermediate_results = store_intermediate_results\n",
    "        if self.store_intermediate_results:\n",
    "            self.stored_eigenvalues = []\n",
    "            self.stored_eigenvectors = []\n",
    "        self.eigen_values = []\n",
    "        self.eigen_vectors = []\n",
    "        self.eigen_value = 0\n",
    "        self.eigen_vector = None\n",
    "            \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def fit(self, A):\n",
    "        \"\"\"\n",
    "        Perform the power method with random initialization, and optionally store\n",
    "        intermediate estimates of the eigenvalue and eigenvectors at each iteration.\n",
    "        You can add an early stopping criterion based on the tolerance parameter.\n",
    "        \"\"\"\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: keep track of your normalization factors, and watch out for passing\n",
    "        # arrays by value vs. by reference. This method should return self\n",
    "        #\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        np.random.seed(self.random_state)\n",
    "        vector = np.random.random(A.shape[0])\n",
    "        vector = vector/np.linalg.norm(vector)\n",
    "        eigen_value = 1\n",
    "        if self.store_intermediate_results:\n",
    "            self.stored_eigenvalues.append(eigen_value)\n",
    "            self.stored_eigenvectors.append(vector)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            vector_previous = np.copy(vector)\n",
    "            vector = A.dot(vector)\n",
    "            eigen_value = np.linalg.norm(vector)\n",
    "            vector = vector/eigen_value\n",
    "            vector = self.gamma * vector_previous + (1 - self.gamma) * vector\n",
    "            if self.store_intermediate_results:\n",
    "                self.stored_eigenvalues.append(eigen_value)\n",
    "                self.stored_eigenvectors.append(vector)\n",
    "            if np.mean(np.sqrt((vector - vector_previous)**2 / vector_previous**2)) < tolerance:\n",
    "                print(\"converged\")\n",
    "                self.eigen_value = eigen_value\n",
    "                self.eigen_vector = vector\n",
    "                break\n",
    "                \n",
    "        self.eigen_value = eigen_value\n",
    "        self.eigen_vector = vector\n",
    "        if self.store_intermediate_results:\n",
    "            self.stored_eigenvalues = np.array(self.stored_eigenvalues)\n",
    "            self.stored_eigenvectors = np.array(self.stored_eigenvectors)\n",
    "        \n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c3957-096f-49a3-a127-1a040723c598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e21592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036574b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12d99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e8aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec70fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa027f6a-5ecd-4a3f-9a4b-7d5ae7cc0345",
   "metadata": {},
   "source": [
    "# Principal Component Analysis and Unsupervised Learning\n",
    "\n",
    "When working with high-dimensional data, it can be helpful to reduce the effective dimensionality of the data by reducing the number of features. For example, suppose we are synthesizing a crystalline material via epitaxy. After preparing a sample, we can measure several properties: reflectivity, conductance, brittleness, etc. After creating a bunch of samples, and measuring several properties for each sample, we want to analyze the relationship among the different features/properties in a systematic way.\n",
    "\n",
    "If we denote a given experiment $i$ with the measurement vector $\\mathbf{x}_i \\in \\mathbb{R}^{N_{feats}}$, then we can represent all of our experiments in a *design matrix* $X \\in \\mathbb{R}^{N_{data} \\times N_{feats}}$, where $N_{data}$ denotes the number of samples or experiments, and $N_{feats}$ represents the number of measurements or features we record for each sample.\n",
    "\n",
    "We know that several of our features are highly correlated across our experiments. Can we find a lower-dimensional representation of our dataset $X' \\in \\mathbb{R}^{N_{data} \\times k}$, $k < N_{feats}$, that describes the majority of variation in our dataset?\n",
    "\n",
    "In principle, reducing the dimensionality of a dataset requires finding an injective function that maps each set of measured features to some lower dimensional set of features,\n",
    "$$\n",
    "\\mathbf{x}' = \\mathbf{f}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "If this function is linear in the features, $\\mathbf{f}(\\mathbf{x}) = C^T \\mathbf{x}$, then this problem reduces to finding the coefficient matrix $C \\in \\mathbb R^{N_{feats} \\times k}$. For this linear case, we can use Principal Component Analysis (PCA).\n",
    "\n",
    "The basic idea of PCA is that the eigenvectors of a dataset's covariance matrix reveal dominant patterns within the dataset, and so by projecting the dataset onto a subset of these eigenvectors, we can find lower-dimensional representation of the data. PCA is optimal in the sense that the first $k$ principal components represent a unique $k$ dimensional representation of a dataset that captures the most variance in the original data. Because PCA is a linear transform (we project the data on a set of basis vectors), then if we project the original dataset onto the full set of $N_{feats}$ eigenvectors, we basically will have rotated our dataset in a high-dimensional vector space, without discarding any information. [More info about PCA here](http://pmaweb.caltech.edu/~physlab/lab_21_current/Ph21_5_Covariance_PCA.pdf)\n",
    "\n",
    "Mathematically, the steps of computing PCA are relatively simple. We center our data by the feature-wise mean vector, compute the covariance matrix, compute the eigenvectors, sort them in descending order of accompanying eigenvalue magnitude, and then stack the first $k$ eigenvectors to create the coefficient matrix\n",
    "$$\n",
    "X = X_r - \\bar{X_r}\n",
    "$$\n",
    "$$\n",
    "\\Sigma = \\dfrac{1}{N_{data}} X^T X\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i = \\text{Sort}[\\text{eig}(\\Sigma)]_i        \n",
    "$$\n",
    "$$\n",
    "C = \\{\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k \\}\n",
    "$$\n",
    "where $\\text{eig}$ denotes the $i^{th}$ eigenvector of $\\Sigma$. The coefficient matrix $C$ denotes the first $k$ eigenvectors stacked on top of each other, sorted in *decreasing* order of the magnitude of their associated eigenvalue. In this context, we call the eigenvalues the \"singular values\" of the original data matrix $X$, while the eigenvectors are the principal components. The central idea of PCA is that the first $k$ principal components (sorted in descending order of eigenvalue magnitude) represent the optimal variance-preserving $k$ dimensional approximation of the original dataset.\n",
    "\n",
    "Our choice of $k \\leq N_{feats}$ will depend on the properties of the dataset. In practice, we usually plot all of the eigenvalues in descending order of magnitude, and then look for a steep dropoff in their average magnitude---this indicates low dimensionality in the underlying dataset. Deciding the value of $k$ determines whether we favor a more concise and compressed representation, or a more accurate one; and various [heuristics exist](https://arxiv.org/abs/1305.5870) for determining the right threshold. After choosing $k$, we discard the remaining eigenvectors.\n",
    "\n",
    "What's nice about PCA is that it generalizes well even to cases where $N_{feats} \\gg N_{data}$. For example, if we are working with high-resolution images, each pixel is essentially a separate feature, and so $N_{feats} = 2048 \\times 2048 \\sim 10^6$.\n",
    "\n",
    "PCA falls broadly under the category of unsupervised learning, which are machine learning techniques that seek to discover structure and patterns in complex data, without external information like annotations that guide the learning process. Instead, unsupervised learning techniques find alternative representations of datasets that satisfy certain desiderata. In the case of PCA, we find a representation of the data in which the first few discovered variables capture the majority of the dataset's variance, thus compressing meaningful properties of the dataset into a lower-dimensional representation.\n",
    "\n",
    "\n",
    "### The von Karman vortex street\n",
    "\n",
    "As an example dataset, we are going to use the velocity field created as a fluid flow passes over a cylinder. At low speeds or high fluid viscousities, the flow parts around the cylinder and then then smoothly rejoins behind it. However, as the speed or viscousity decreases, and instability appears in which the flow begins to oscillate behind the cylinder, giving rise to sets of counter-rotating vortices that break off the cylinder and pass into the wake. As speed further increases, this sequence of vortices becomes more chaotic, leading to irregular downstream structure.\n",
    "\n",
    "The control parameter governing this transition from ordered to chaotic flow is the Reynolds number, a dimensionless parameter directly proportional to the speed, and inversely proportional to the viscousity. The Reynolds number arises from non-dimensionalizing the Navier-Stokes equations, and it measures the relative ratio of inertial forces to viscous forces in the system. In general, flows become more turbulent at higher Reynolds numbers.\n",
    "\n",
    "In the class repository, we have included very large datasets corresponding to time series of snapshots showing the velocity field of the flow past a cylinder. We include separate datasets from several different Reynolds numbers, which show how the structure of the von Karman instability changes as the flow becomes more turbulent.\n",
    "\n",
    "In order to better understand the structure of this unstable fluid flow, we are going to implement PCA, and apply it to these snapshots.\n",
    "\n",
    "*For this example problem, you will need to download the [Von Karman Vortex Street Dataset](https://utexas.box.com/s/44f89zfy7v2k4g5wq4kv3vfvkm9w91wm) from Box. Place the downloaded files into the subdirectory `cphy/resources/von_karman_street/`. Alterntatively, you can manually edit the path we use below to import the dataset.*\n",
    "\n",
    "### TO DO\n",
    "\n",
    "+ Explore the included von Karman datasets using the code below. What symmetries are present in the data? Do any symmetries change as we increase the Reynolds number?\n",
    "+ Implement Principal Component Analysis in Python. I have included my outline code below; we are going to use multiple inheritence in order to make our implementation compatible with standard conventions for machine learning in Python. You can use numpy's built-in eigensystem solvers `np.linalg.eig` and `np.linalg.eigh`\n",
    "+ Plot the eigenvalues of the data covariance matrix in descending order. What does this tell us about the effective dimensionality, and thus optimal number of features, to use to represent the von Karman dataset?\n",
    "+ Try re-running your analysis using datasets from different Reynolds numbers. How does the effective dimensionality of the problem change as Reynolds number increases?\n",
    "+ For this problem, the principal components often appear in pairs. Can you think of a reason for this?\n",
    "+ What happens if we don't subtract the feature-wise mean before calculating PCA?\n",
    "+ In Fourier analysis, we project a function onto linear combination of trigonometric basis functions. How is this related to principal component analysis?\n",
    "\n",
    "### Fun Facts\n",
    "\n",
    "+ The flow field we are studying was simulated using [Lattice Boltzmann methods](http://www.scholarpedia.org/article/Lattice_Boltzmann_Method). In the next part of the course, we will learn how to numerically solve partial differential equations such as the Navier-Stokes equations that govern fluid flows. Lattice Boltzmann methods are distinct from the solvers we will use, however, because they involve simulating individual particles rather than a governing equation. LBM have significant similarity to the cellular automata we implemented previously\n",
    "+ We are applying PCA to sequences of velocity fields from a fluid flow, but this method can be applied to images, graphs, and almost any high-dimensional dataset. All that really matters is that you can take your data, and apply some sort of invertible transformation so that it becomes a big matrix of shape $(N_{samples}, N_{features})$\n",
    "+ What about the more general problem of finding new features are that nonlinear functions of our observed features? One option would be to transform the data with fixed nonlinear functions that capture important features, such as trigonometric functions or spatially-localized kernels, and then apply the PCA calculation in this \"lifted\" space. This approach is the basis of kernel-PCA. Even more general approaches are the subject of current research in nonlinear embedding techniques, such as [UMAP and tSNE](https://pair-code.github.io/understanding-umap/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e58c1d6",
   "metadata": {},
   "source": [
    "### Load and explore the raw velocity field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Re = 1200 # Reynolds number, change this to 300, 600, 900, 1200\n",
    "\n",
    "# Load the two-dimensional velocity field data. Data is stored in a 4D numpy array,\n",
    "# where the first dimension is the time index, the second and third dimensions are the\n",
    "# x and y coordinates, and the fourth dimension is the velocity components (ux or uv).\n",
    "vfield = np.load(\n",
    "    f\"../resources/von_karman_street/vortex_street_velocities_Re_{Re}_largefile.npz\", \n",
    "    allow_pickle=True\n",
    ")\n",
    "print(\"Velocity field data has shape: {}\".format(vfield.shape))\n",
    "\n",
    "# Calculate the vorticity, which is the curl of the velocity field\n",
    "## Missing minus sign??\n",
    "vort_field = np.diff(vfield, axis=1)[..., :-1, 1] + np.diff(vfield, axis=2)[:, :-1, :, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e79c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams['animation.embed_limit'] = 2**26\n",
    "\n",
    "# Assuming frames is a numpy array with shape (num_frames, height, width)\n",
    "frames = vort_field.copy()[::150]\n",
    "vmax = np.percentile(np.abs(frames), 98)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "img = plt.imshow(frames[0], vmin=-vmax, vmax=vmax, cmap=\"RdBu\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "# tight margins\n",
    "plt.margins(0,0)\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "def update(frame):\n",
    "    img.set_array(frame)\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=frames, interval=100)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829db26",
   "metadata": {},
   "source": [
    "### Implement principal component analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc29e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# We are going to use class inheritance to define our object. The two base classes from\n",
    "# scikit-learn represent placeholder objects for working with datasets. They include \n",
    "# many generic methods, like fetching parameters, getting the data shape, etc.\n",
    "# \n",
    "# By inheriting from these classes, we ensure that our object will have access to these\n",
    "# functions, even though we don't have to define them ourselves. Earlier in the course\n",
    "# we saw examples where we defined our own template classes. Here, we are using the\n",
    "# template classes define by the scikit-learn Python library.\n",
    "class PrincipalComponents(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A class for performing principal component analysis on a dataset.\n",
    "\n",
    "    Parameters\n",
    "        random_state (int): random seed for reproducibility\n",
    "        components_ (numpy array): the principal components\n",
    "        singular_values_ (numpy array): the singular values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_state=None):\n",
    "        self.random_state = random_state\n",
    "        self.components_ = None\n",
    "        self.singular_values_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data X. Store the eigenvectors in the attribute\n",
    "        self.components_ and the eigenvalues in the attribute self.singular_values_\n",
    "\n",
    "        NOTE: This method needs to return self in order to work properly with the\n",
    "         scikit-learn base classes from which it inherits.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the\n",
    "                data to be fit.\n",
    "        \n",
    "        Returns:\n",
    "            self (PrincipalComponents): The fitted object.\n",
    "        \"\"\"\n",
    "\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        #\n",
    "        # # YOUR CODE HERE\n",
    "        # # Hint: Keep track of whether you should be multiplying by a matrix or\n",
    "        # # its transpose.\n",
    "        #\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        \n",
    "        X_shifted = X - np.mean(X)\n",
    "        covariance_matrix = X_shifted.T.dot(X_shifted) / X_shifted.shape[0]\n",
    "        S, V = np.linalg.eigh(covariance_matrix)\n",
    "        indices = np.argsort(S)[::1]\n",
    "        V = V.T\n",
    "        singular_values = []\n",
    "        components = []\n",
    "        for i in range(len(indices)):\n",
    "            singular_values.append(S[indices[i]])\n",
    "            components.append(V[indices[i]])\n",
    "        self.components_ = components\n",
    "        self.singular_values_ = singular_values\n",
    "        return self\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data X into the new basis using the PCA components.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): A 2D array of shape (n_samples, n_features) containing the\n",
    "                data to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            X_new (np.ndarray): A 2D array of shape (n_samples, n_components) containing\n",
    "                the transformed data. n_components <= n_features, depending on whether\n",
    "                we truncated the eigensystem.\n",
    "        \"\"\"\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        #\n",
    "        # # YOUR CODE HERE\n",
    "        #\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        X_shifted = X - np.mean(X)\n",
    "        return X_shfited.dot(self.components_.T)\n",
    "#        raise NotImplementedError()\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform from principal components space back to the original space\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): A 2D array of shape (n_samples, n_components) containing the\n",
    "                data to be transformed. n_components <= n_features, depending on whether\n",
    "                we truncated the eigensystem.\n",
    "\n",
    "        Returns:\n",
    "            X_new (np.ndarray): A 2D array of shape (n_samples, n_features) containing\n",
    "                the transformed data.\n",
    "        \"\"\"\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        #\n",
    "        # # YOUR CODE HERE\n",
    "        #\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        return X.dot(self.components_) + np.mean(X, axis=0)\n",
    "\n",
    "    ## You shouldn't need to implement this, because it gets inherited from the base\n",
    "    ## class. But if you are having trouble with the inheritance, you can uncomment\n",
    "    ## this and to implement it.\n",
    "    # def fit_transform(self, X):\n",
    "    #     self.fit(X)\n",
    "    #     return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d946f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load William's solutions\n",
    "# from solutions.pca import PrincipalComponents\n",
    "\n",
    "data = np.copy(vort_field)[::3, ::2, ::2] # subsample data to reduce compute load\n",
    "data_reshaped = np.reshape(data, (data.shape[0], -1))\n",
    "\n",
    "model = PrincipalComponents()\n",
    "# model = PCA()\n",
    "\n",
    "data_transformed = model.fit_transform(data_reshaped)\n",
    "principal_components = np.reshape(\n",
    "    model.components_, (model.components_.shape[0], data.shape[1], data.shape[2])\n",
    ")\n",
    "\n",
    "## Look at skree plot, and identify the \"elbow\" indicating low dimensionality\n",
    "plt.figure()\n",
    "plt.plot(model.singular_values_[:50])\n",
    "plt.plot(model.singular_values_[:50], '.')\n",
    "plt.xlabel(\"Eigenvalue magnitude\")\n",
    "plt.ylabel(\"Eigenvalue rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    vscale = np.percentile(np.abs(principal_components[i]), 99)\n",
    "    plt.imshow(principal_components[i], cmap=\"RdBu\", vmin=-vscale, vmax=vscale)\n",
    "    plt.title(\"PC {}\".format(i+1))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    plt.subplot(8, 1, i+1)\n",
    "    plt.plot(data_transformed[:, i])\n",
    "    plt.ylabel(\"PC {} Amp\".format(i+1))\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot(data_transformed[:, 0], data_transformed[:, 1], data_transformed[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169637b0-6a6e-4e14-852a-d6115f698300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b4ee79",
   "metadata": {},
   "source": [
    "### Extras\n",
    "\n",
    "This is the code William uses to make videos of his plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Re = 1200 # Reynolds number, change this to 300, 600, 900, 1200\n",
    "\n",
    "# Load the two-dimensional velocity field data. Data is stored in a 4D numpy array,\n",
    "# where the first dimension is the time index, the second and third dimensions are the\n",
    "# x and y coordinates, and the fourth dimension is the velocity components (ux or uv).\n",
    "vfield = np.load(\n",
    "    f\"../resources/von_karman_street/vortex_street_velocities_Re_{Re}_largefile.npz\", \n",
    "    allow_pickle=True\n",
    ")\n",
    "print(\"Velocity field data has shape: {}\".format(vfield.shape))\n",
    "\n",
    "# Calculate the vorticity, which is the curl of the velocity field\n",
    "vort_field = np.diff(vfield, axis=1)[..., :-1, 1] + np.diff(vfield, axis=2)[:, :-1, :, 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a1f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48802e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f69bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0fe123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c9f12ae",
   "metadata": {},
   "source": [
    "## Future\n",
    "\n",
    "*These are some ideas for future versions of this assignment. They are not part of the homework*\n",
    "\n",
    "\n",
    "### Random matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c199b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_rot_scale(a):\n",
    "    \"\"\"\n",
    "    Decompose a matrix into translation, skew, rotation, and scale components\n",
    "    \"\"\"\n",
    "    # Translation\n",
    "    t = a[:, -1]\n",
    "    # Rotation and scale\n",
    "    r = a[:-1, :-1]\n",
    "    # Skew\n",
    "    s = 0.5 * (r - r.T)\n",
    "    # Scale\n",
    "    c = 0.5 * (r + r.T)\n",
    "    return t, s, c\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random(size=(10, 10))\n",
    "a = np.random.normal(size=(10, 10))\n",
    "\n",
    "# a = np.random.random(size=(500, 500))\n",
    "# a = np.random.normal(size=(500, 500))\n",
    "# a = a + a.T\n",
    "# a = a - np.mean(a, axis=1)\n",
    "t, s, c = skew_rot_scale(a)\n",
    "\n",
    "# print(np.linalg.eig(a)[0])\n",
    "# print(np.linalg.norm(c))\n",
    "# print(np.linalg.det(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Girko-Ginibri circular law\n",
    "\n",
    "a = np.random.random(size=(500, 500))\n",
    "a = a - np.mean(a, axis=1)\n",
    "\n",
    "# a = np.random.normal(size=(500, 500))\n",
    "\n",
    "# a = a + a.T\n",
    "\n",
    "eig = np.linalg.eig(a)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(eig.real, eig.imag, '.k', markersize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0e936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509824da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wigner's Semicircle Law\n",
    "\n",
    "a = np.random.normal(size=(1000, 1000))\n",
    "a = a + a.T\n",
    "\n",
    "eig = np.linalg.eig(a)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(eig, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make an interactive video (optional; requires ipywidgets and has some RAM overhead)\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "frames = vort_field[::10]\n",
    "\n",
    "vscale = np.percentile(np.abs(vort_field), 98)\n",
    "def plotter(i):\n",
    "    plt.close()\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(\n",
    "        frames[i],\n",
    "        vmin=-vscale, vmax=vscale, cmap=\"RdBu\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plotter, \n",
    "    i=widgets.IntSlider(0, 0, len(frames) - 1, 1, layout=Layout(width='500px'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vort_field2 = vort_field[::5][:500]\n",
    "vscale = np.percentile(np.abs(vort_field2), 95)\n",
    "\n",
    "for i in range(len(vort_field2) - 1):\n",
    "    \n",
    "    \n",
    "    out_path = \"private_dump/wake/frame\" + str(i).zfill(4) + \".png\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(vort_field2[i], cmap=\"RdBu\", vmin=-vscale, vmax=vscale)\n",
    "\n",
    "    plt.xlim([0, vort_field2.shape[-1]])\n",
    "    plt.ylim([60, 60 + vort_field2.shape[-1]])\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.set_aspect(1, adjustable='box')\n",
    "\n",
    "    plt.savefig(out_path, bbox_inches='tight', pad_inches=0.0, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29318ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717962b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e972983abb2b5c6293c34082f6ff1f6e60e8afbd2a068e0026ccecbb212fdb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
